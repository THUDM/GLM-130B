WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING: No training data specified
WARNING: No training data specified
WARNING: No training data specified
WARNING: No training data specified
WARNING: No training data specified
WARNING: No training data specified
using world size: 8 and model-parallel size: 8 
> padded vocab (size: 150528) with 0 dummy tokens (new size: 150528)
WARNING: No training data specified
WARNING: No training data specified
> initializing model parallel with size 8
> Loading task configs
    Task StereoSet loaded from config tasks/ethnic/StereoSet.yaml
> Successfully load 1 task
> Set tokenizer as a icetk-glm-130B tokenizer! Now you can get_tokenizer() everywhere.
